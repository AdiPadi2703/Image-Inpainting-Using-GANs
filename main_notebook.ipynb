{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ba5fe-f047-4657-b362-8e5f502b6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Inpainting Using GANs\n",
    "# This notebook contains both the training and testing of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a7a78-0ce5-480e-acf2-2fa37cb07458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv2d, ConvTranspose2d, BatchNorm2d, Dropout, LeakyReLU, ReLU, Linear, Flatten, Tanh, InstanceNorm2d\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bb084-0172-41c0-9cc5-e00687e0d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501cb62-cbcc-4bdb-ba55-b186992ac343",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0c685-442f-4213-9921-62fca1cad818",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397bc45-1fd9-4e3e-b09e-f8fb4de10ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256\n",
    "MASK_SIZE = 128\n",
    "BATCH_SIZE = 2\n",
    "TEST_SIZE = 64\n",
    "NUM_CHANNELS = 3\n",
    "LEARNING_RATE_DISC = 0.0001\n",
    "LEARNING_RATE_GEN = 0.0001\n",
    "EPOCHS = 30\n",
    "LAMBDA_AD = 0.01\n",
    "LAMBDA_R = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a48ad-117b-467c-88c9-093e4a9e844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"archive/img_align_celeba/img_align_celeba/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6b1b4-f507-497b-a43f-2dac44e9b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image):\n",
    "\n",
    "    # Function to add the square patches to the images\n",
    "\n",
    "    x1,y1 = np.random.randint(0, (IMG_SIZE - MASK_SIZE) , 2)\n",
    "    x2,y2 = x1 + MASK_SIZE, y1 + MASK_SIZE\n",
    "\n",
    "    mask = np.zeros((1, IMG_SIZE,IMG_SIZE), dtype=np.float32)\n",
    "    mask[: ,y1:y2,x1:x2] = 1.\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853e696-38dc-46d5-ad75-40282f741b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.files = sorted(os.listdir(root_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.files[idx]\n",
    "        image_path = self.root_dir + image_path\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        image = np.array(image, dtype = np.float32)\n",
    "        image = image / 255\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "            mask = augment(image)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1a3a0-78a5-4048-a7d8-887a1223170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "\n",
    "transforms = Compose([ToTensor(), Resize((IMG_SIZE, IMG_SIZE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c498637-fc0e-4e4b-ade9-b5c7e7afc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(root_dir,transforms)\n",
    "train_size = dataset.__len__() - TEST_SIZE\n",
    "test_size = TEST_SIZE\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size,test_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size= 1, shuffle=test_dataset.__len__(), num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26a1a-83d4-424a-91b1-d391bdd1504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(image,mask) in enumerate(test_dataloader):\n",
    "\n",
    "    # Save test images and masks in a separate directory\n",
    "\n",
    "    to_pil = torchvision.transforms.ToPILImage()\n",
    "    img_pil = to_pil(image[0])\n",
    "    img_pil.save(f'TestingImages/img{i}.png')\n",
    "    mask_pil = to_pil(mask[0])\n",
    "    mask_pil.save(f'TestingMasks/img{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e43e3-b720-4a32-bacd-042d21f3aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_dataloader))\n",
    "sample_idx = random.randint(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d4056-e1c4-4239-8fef-5abbfdc9666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the masks for the images and the dataset are fine\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(sample[0][sample_idx].permute(1,2,0))\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(sample[1][sample_idx].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdc1c9-2eb8-4a91-bc7b-b2cbbf1350e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels=3, cnum=64):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        \n",
    "        self.disc = nn.Sequential(\n",
    "            \n",
    "            ConvSN(in_channels, cnum),\n",
    "            ConvSN(cnum, 2*cnum),\n",
    "            ConvSN(2*cnum, 4*cnum),\n",
    "            ConvSN(4*cnum, 4*cnum),\n",
    "            ConvSN(4*cnum, 4*cnum),\n",
    "            ConvSN(4*cnum, 4*cnum),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310aa07-3cd0-44fa-8887-fcb1869787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, cnum_in = 3, cnum= 64, cnum_out=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.coarse_net = nn.Sequential(\n",
    "            \n",
    "            GatedConv(cnum_in+2, cnum//2, kernel_size=5, stride=1, padding=2),\n",
    "    \n",
    "           \n",
    "            DownSample(cnum//2, cnum),\n",
    "            DownSample(cnum, 2*cnum),\n",
    "    \n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, stride=1),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=2, padding=2),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=4, padding=4),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=8, padding=8),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=16, padding=16),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, stride=1),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, stride=1),\n",
    "    \n",
    "            UpSample(2*cnum, cnum),\n",
    "            UpSample(cnum, cnum//4, cnum//2),\n",
    "    \n",
    "            nn.Conv2d(cnum//4, cnum_out, kernel_size=3, stride=1, padding = \"same\"),\n",
    "                                    \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.refine_down = nn.Sequential(\n",
    "\n",
    "            GatedConv(cnum_in, cnum//2, kernel_size=5, stride=1, padding=2),\n",
    "    \n",
    "           \n",
    "            DownSample(cnum//2, cnum),\n",
    "            DownSample(cnum, 2*cnum),\n",
    "    \n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, stride=1),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=2, padding=2),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=4, padding=4),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=8, padding=8),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, rate=16, padding=16)\n",
    "        )\n",
    "\n",
    "        self.attention = SelfAttention(2*cnum, \"relu\")\n",
    "\n",
    "        self.refine_up = nn.Sequential(\n",
    "            \n",
    "            GatedConv(2*2*cnum, 2*cnum, kernel_size=3, stride=1),\n",
    "            GatedConv(2*cnum, 2*cnum, kernel_size=3, stride=1),\n",
    "    \n",
    "            UpSample(2*cnum, cnum),\n",
    "            UpSample(cnum, cnum//4, cnum//2),\n",
    "    \n",
    "            nn.Conv2d(cnum//4, cnum_out, kernel_size=3, stride=1, padding = \"same\"),\n",
    "                                    \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x_ = x\n",
    "        ones_x = torch.ones_like(mask)[:, 0:1].to(device)\n",
    "        x = torch.cat([x, ones_x, ones_x * mask], dim = 1)\n",
    "        x_coarse = self.coarse_net(x)\n",
    "\n",
    "        x2 = x_coarse * mask + x_\n",
    "        x_conv = self.refine_down(x2)\n",
    "        x_att = self.refine_down(x2)\n",
    "        x_att = self.attention(x_att)\n",
    "        x_cat = torch.cat([x_conv, x_att], dim =1)\n",
    "        x_refine = self.refine_up(x_cat)\n",
    "        \n",
    "        return x_refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749207a-5c81-4d2e-ac9c-f112c96eceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_disc = Discriminator(in_channels=3).to(device)\n",
    "gen = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c832ea-7006-44ca-9caf-1e9bd92e814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(gen, [(3,IMG_SIZE,IMG_SIZE), (1, IMG_SIZE, IMG_SIZE)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21430b-5ef9-4c64-a78e-75fd8862aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60563bc-348c-4275-94c0-15061411c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_weights(global_disc)\n",
    "initialize_weights(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc853de1-6555-4f41-905f-fe30eaa8d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_disc.load_state_dict(torch.load('disc.pth'))\n",
    "# gen.load_state_dict(torch.load('gen.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f0e11-1873-485f-bc9b-6c2c9c44a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = torch.optim.Adam(gen.parameters(), lr = LEARNING_RATE_GEN, betas= (0.5, 0.999))\n",
    "global_opt = torch.optim.Adam(global_disc.parameters(), lr = LEARNING_RATE_DISC, betas= (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482e7fc-3423-4ff9-8b5f-079435ce5435",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "global_scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395e06d-b5fd-4130-8bf0-204911993410",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_LOSS = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91603ed-d64b-4735-97ab-a7d57b608fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, global_disc, gen , global_opt, gen_opt, global_scaler, g_scaler, l1):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "        gen.train()\n",
    "        global_disc.train()\n",
    "        targets = batch[0].to(device)\n",
    "        masks = batch[1].to(device)\n",
    "\n",
    "        x_t = targets * (1 - masks).float() + masks\n",
    "        x_pred = gen(x_t, masks)\n",
    "    \n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            fake_global = global_disc(x_pred.detach())\n",
    "            real_global = global_disc(targets)\n",
    "    \n",
    "            global_loss = torch.mean(torch.relu(torch.ones_like(real_global).to(device) - real_global)) + torch.mean(torch.relu(torch.ones_like(fake_global).to(device) + fake_global))\n",
    "    \n",
    "        global_disc.zero_grad()\n",
    "        global_scaler.scale(global_loss).backward(retain_graph=True)\n",
    "        global_scaler.step(global_opt)\n",
    "        global_scaler.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            fake_gen = global_disc(x_pred)\n",
    "            adversarial_loss = -torch.mean(torch.relu(torch.ones_like(fake_gen).to(device) + fake_gen))\n",
    "            recon_loss = l1(x_pred, targets)\n",
    "            gen_loss = LAMBDA_AD* adversarial_loss + LAMBDA_R * recon_loss \n",
    "\n",
    "        gen.zero_grad()\n",
    "        g_scaler.scale(gen_loss).backward()\n",
    "        g_scaler.step(gen_opt)\n",
    "        g_scaler.update()\n",
    "\n",
    "        count += 1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Generator Loss : {gen_loss} Global Loss : {global_loss}\")\n",
    "            gen.eval()\n",
    "            plt.subplot(2, 2, 1)\n",
    "            x = ((1 - sample[1][sample_idx])*sample[0][sample_idx] + sample[1][sample_idx]).expand((1,3,IMG_SIZE,IMG_SIZE)).to(device)\n",
    "            sample_pred = gen(x, sample[1][sample_idx].expand((1,1,IMG_SIZE,IMG_SIZE)).to(device))\n",
    "            plt.imshow(sample_pred[0].permute(1,2,0).cpu().detach().numpy())\n",
    "            plt.show()\n",
    "\n",
    "        if count % 10000 == 0:\n",
    "\n",
    "            torch.save(gen.state_dict(), f\"gen{count}.pth\")\n",
    "            torch.save(global_disc.state_dict(),f\"disc{count}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b07dd-71bf-4c5d-a4da-51f123297209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"EPOCH {epoch+1}:\")\n",
    "    train_loop(train_dataloader, global_disc, gen ,  global_opt, gen_opt, global_scaler, g_scaler, L1_LOSS)\n",
    "    \n",
    "    torch.save(gen.state_dict(), \"Model/gen.pth\")\n",
    "    torch.save(global_disc.state_dict(),\"Model/disc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ca0db-d509-4aa3-9a65-57b7d132fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15dc4e-348d-4ad0-a9e9-786f71efa723",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator().to(device)\n",
    "disc = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c7bf2-475e-4fdd-b8aa-ecba3b75de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.load_state_dict(torch.load('Model/gen.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a107aa-522c-4f88-8301-a6343a52492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(os.listdir('TestingImages/'))\n",
    "masks = sorted(os.listdir('TestingMasks/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d3fe6-3629-47bf-9255-6410315846b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_masked_images = []\n",
    "test_predictions = []\n",
    "\n",
    "for i in range(len(images)):\n",
    "\n",
    "    image = images[i]\n",
    "    mask = masks[i]\n",
    "    image = np.array(Image.open('TestingImages/' + image))\n",
    "    mask = np.array(Image.open('TestingMasks/' + mask))\n",
    "\n",
    "    test_images.append(image)\n",
    "    \n",
    "    image = torchvision.transforms.Compose([ToTensor()])(image).expand((1,3,IMG_SIZE,IMG_SIZE)).to(device)\n",
    "    mask = torchvision.transforms.Compose([ToTensor()])(mask).expand((1,1,IMG_SIZE,IMG_SIZE)).to(device)\n",
    "\n",
    "    masked_image = image * (1 - mask).float() + mask\n",
    "\n",
    "    test_masked_images.append(masked_image[0].permute(1,2,0).cpu().detach().numpy())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen.eval()\n",
    "\n",
    "        pred = gen(masked_image, mask)\n",
    "\n",
    "        pred_image = pred * mask + image * (1 - mask).float()\n",
    "        pred_image = pred_image[0].permute(1,2,0).cpu().detach().numpy()\n",
    "        test_predictions.append(pred_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9280ed-0f82-494a-9099-308445a6850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(test_images)):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.imshow(test_images[index])\n",
    "    plt.title('Original Image')\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.subplot(2,3,2)\n",
    "    plt.imshow(test_masked_images[index])\n",
    "    plt.title('Masked Image')\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.imshow(test_predictions[index])\n",
    "    plt.title('Inpainted Image')\n",
    "    plt.axis(False)\n",
    "\n",
    "    plt.savefig(f'Results/img{index}')\n",
    "\n",
    "    plt.close()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df4b74-c0ca-4e05-8340-0d1e37dcac95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
